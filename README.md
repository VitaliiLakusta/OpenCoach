# OpenCoach

A minimal prototype chat application with Next.js, Vercel AI SDK, and Mastra (to be integrated).

## Features

âœ¨ **Multi-Model Support** - Choose from OpenAI, Anthropic, Google, Mistral, or **local models**  
ğŸ  **Local LLMs** - Run Llama, Mistral, and other models locally via Ollama  
ğŸ“ **Notes Integration** - Read and write to your local note files  
ğŸ“… **Calendar Integration** - Connect your calendar for scheduling and planning  
ğŸ’¬ **Streaming Responses** - Real-time AI responses with Vercel AI SDK  

## Setup

1. Install dependencies:
```bash
npm install
```

2. Set up environment variables:
Create a `.env.local` file with your API keys (optional for local models):
```
OPENAI_API_KEY=your_api_key_here
ANTHROPIC_API_KEY=your_api_key_here  # Optional
GOOGLE_API_KEY=your_api_key_here     # Optional
```

3. (Optional) Set up local models with Ollama:

**Why use local models?**
- ğŸ”’ Complete privacy - your data never leaves your machine
- ğŸ’° Zero API costs - unlimited usage
- âš¡ Often faster than cloud APIs
- ğŸŒ Works offline

**Install Ollama:**

macOS & Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Windows:
- Download from [ollama.ai](https://ollama.ai)

**Download models:**
```bash
# Recommended starter (4GB - good balance of speed and quality)
ollama pull llama3.2

# Fastest, most lightweight (2GB - great for quick responses)
ollama pull phi3

# Most powerful (7GB - best quality, needs 16GB+ RAM)
ollama pull llama3.1

# Other popular options
ollama pull mistral      # 4GB - balanced performance
ollama pull codellama    # 4GB - specialized for code
ollama pull gemma2       # 3GB - creative writing
```

**Verify installation:**
```bash
# Check Ollama is running
ollama list

# You should see your downloaded models
```

**Use in OpenCoach:**
1. Start OpenCoach (see step 4 below)
2. Click the settings icon (âš™ï¸)
3. Select a local model from the dropdown (marked with "Local")
4. Start chatting privately and for free!

ğŸ’¡ **Tip**: Try `phi3` first for the fastest experience, or `llama3.2` for better quality responses.

ğŸ“– For troubleshooting and advanced configuration, see [LOCAL_MODELS.md](./LOCAL_MODELS.md)

4. Run the development server:
```bash
npm run dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## Using Local Models

OpenCoach supports running LLMs locally for privacy and cost savings!

### Available Local Models

| Model | Size | Speed | Best For | RAM Needed |
|-------|------|-------|----------|------------|
| **Phi-3** | 2GB | âš¡âš¡âš¡ Very Fast | Quick questions, TODOs | 8GB |
| **Llama 3.2** | 4GB | âš¡âš¡ Fast | Daily journaling, coaching | 8GB |
| **Mistral** | 4GB | âš¡âš¡ Fast | Balanced performance | 8GB |
| **Gemma 2** | 3GB | âš¡âš¡ Fast | Creative writing | 8GB |
| **Code Llama** | 4GB | âš¡âš¡ Fast | Technical help, coding | 8GB |
| **Llama 3.1** | 7GB | âš¡ Medium | Complex reasoning | 16GB |

### Quick Troubleshooting

**"Connection refused" error?**
```bash
ollama serve  # Start Ollama manually
```

**Model not showing up?**
```bash
ollama list              # Check installed models
ollama pull llama3.2     # Download if missing
```

**Running slow?**
- Try a smaller model: `ollama pull phi3`
- Close other applications
- Ensure 8GB+ RAM available

**Still having issues?** See [DEBUGGING_LOCAL_MODELS.md](./DEBUGGING_LOCAL_MODELS.md) for comprehensive debugging steps.

### Local vs Cloud Models

**Use Local Models for:**
- ğŸ“ Personal journaling and reflections
- âœ… TODO lists and daily planning
- ğŸ¤” Quick questions and brainstorming
- ğŸ”’ Sensitive or private information

**Use Cloud Models for:**
- ğŸ§  Complex reasoning and analysis
- ğŸ“Š Large documents and contexts
- ğŸš€ Latest AI capabilities
- ğŸ¯ Critical decisions

ğŸ“š **For complete documentation, see [LOCAL_MODELS.md](./LOCAL_MODELS.md)**

## Current Features

- âœ… Multi-provider AI model support (OpenAI, Anthropic, Google, Mistral, Ollama)
- âœ… Local LLM support via Ollama
- âœ… Basic chat UI using Vercel AI SDK's `useChat` hook
- âœ… Streaming responses from the backend
- âœ… Notes folder integration (read/write)
- âœ… Calendar integration (iCal format)
- âœ… TODO management
- âœ… Event creation with calendar links

## Next Steps

- [ ] Integrate Mastra agent for advanced reasoning and tools
- [ ] Implement space concept and context files (CONTEXT.md)
- [ ] Add scheduled reminders and notifications
- [ ] Build multi-space support
- [ ] Enhanced note organization and search


