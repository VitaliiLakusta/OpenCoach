# OpenCoach

A minimal prototype chat application with Next.js, Vercel AI SDK, and Mastra (to be integrated).

## Getting Started

You have two options to run OpenCoach:

**Option 1: Run the Pre-built Desktop App** (Recommended for most users)
- Download the pre-built application from the [dist/](dist/) directory
- macOS: Use `OpenCoach-0.1.0-universal.dmg` or the `.zip` file
- Windows: Use `OpenCoach Setup 0.1.0.exe` or the portable `.exe` file
- No setup required - just download and run!

**Option 2: Run Locally from Source** (For developers)
- Clone/checkout this project
- Follow the setup instructions below to run the development server
- Allows you to modify and customize the application

## Features

‚ú® **Multi-Model Support** - Choose from OpenAI, Anthropic, Google, Mistral, or **local models**  
üè† **Local LLMs** - Run Llama, Mistral, and other models locally via Ollama  
üìù **Notes Integration** - Read and write to your local note files  
üìÖ **Calendar Integration** - Connect your calendar for scheduling and planning  
üí¨ **Streaming Responses** - Real-time AI responses with Vercel AI SDK  

## Setup

1. Install dependencies:
```bash
npm install
```

2. Set up API keys:

You have two options for configuring your OpenAI API key:

**Option A: Via UI (Recommended)**
- Start the app and click on "‚öôÔ∏è Configuration"
- Enter your OpenAI API key in the "OpenAI API Key" field
- Your key is stored securely in your browser's local storage

**Option B: Via Environment Variables**
Create a `.env.local` file with your API keys:
```
OPENAI_API_KEY=your_api_key_here      # Optional if configured in UI
ANTHROPIC_API_KEY=your_api_key_here   # Optional
GOOGLE_API_KEY=your_api_key_here      # Optional
```

Note: Keys configured in the UI take precedence over environment variables.

3. (Optional) Set up local models with Ollama:

**Why use local models?**
- üîí Complete privacy - your data never leaves your machine
- üí∞ Zero API costs - unlimited usage
- ‚ö° Often faster than cloud APIs
- üåê Works offline

**Install Ollama:**

macOS & Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

Windows:
- Download from [ollama.ai](https://ollama.ai)

**Download models:**
```bash
# Recommended starter (4GB - good balance of speed and quality)
ollama pull llama3.2

# Fastest, most lightweight (2GB - great for quick responses)
ollama pull phi3

# Most powerful (7GB - best quality, needs 16GB+ RAM)
ollama pull llama3.1

# Other popular options
ollama pull mistral      # 4GB - balanced performance
ollama pull codellama    # 4GB - specialized for code
ollama pull gemma2       # 3GB - creative writing
```

**Verify installation:**
```bash
# Check Ollama is running
ollama list

# You should see your downloaded models
```

**Use in OpenCoach:**
1. Start OpenCoach (see step 4 below)
2. Click the settings icon (‚öôÔ∏è)
3. Select a local model from the dropdown (marked with "Local")
4. Start chatting privately and for free!

üí° **Tip**: Try `phi3` first for the fastest experience, or `llama3.2` for better quality responses.

üìñ For troubleshooting and advanced configuration, see [LOCAL_MODELS.md](./LOCAL_MODELS.md)

4. Run the development server:
```bash
npm run dev
```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## Using Local Models

OpenCoach supports running LLMs locally for privacy and cost savings!

### Available Local Models

| Model | Size | Speed | Best For | RAM Needed |
|-------|------|-------|----------|------------|
| **Phi-3** | 2GB | ‚ö°‚ö°‚ö° Very Fast | Quick questions, TODOs | 8GB |
| **Llama 3.2** | 4GB | ‚ö°‚ö° Fast | Daily journaling, coaching | 8GB |
| **Mistral** | 4GB | ‚ö°‚ö° Fast | Balanced performance | 8GB |
| **Gemma 2** | 3GB | ‚ö°‚ö° Fast | Creative writing | 8GB |
| **Code Llama** | 4GB | ‚ö°‚ö° Fast | Technical help, coding | 8GB |
| **Llama 3.1** | 7GB | ‚ö° Medium | Complex reasoning | 16GB |

### Quick Troubleshooting

**"Connection refused" error?**
```bash
ollama serve  # Start Ollama manually
```

**Model not showing up?**
```bash
ollama list              # Check installed models
ollama pull llama3.2     # Download if missing
```

**Running slow?**
- Try a smaller model: `ollama pull phi3`
- Close other applications
- Ensure 8GB+ RAM available

**Still having issues?** See [DEBUGGING_LOCAL_MODELS.md](./DEBUGGING_LOCAL_MODELS.md) for comprehensive debugging steps.

### Local vs Cloud Models

**Use Local Models for:**
- üìù Personal journaling and reflections
- ‚úÖ TODO lists and daily planning
- ü§î Quick questions and brainstorming
- üîí Sensitive or private information

**Use Cloud Models for:**
- üß† Complex reasoning and analysis
- üìä Large documents and contexts
- üöÄ Latest AI capabilities
- üéØ Critical decisions

üìö **For complete documentation, see [LOCAL_MODELS.md](./LOCAL_MODELS.md)**

## Current Features

- ‚úÖ Multi-provider AI model support (OpenAI, Anthropic, Google, Mistral, Ollama)
- ‚úÖ Local LLM support via Ollama
- ‚úÖ **Desktop Apps for macOS & Windows** - Native Electron apps available!
- ‚úÖ Basic chat UI using Vercel AI SDK's `useChat` hook
- ‚úÖ Streaming responses from the backend
- ‚úÖ Notes folder integration (read/write)
- ‚úÖ Calendar integration (iCal format)
- ‚úÖ TODO management
- ‚úÖ Event creation with calendar links

## Desktop App (Electron)

OpenCoach is available as a native desktop application for **macOS** and **Windows**!

### Download

Pre-built installers are available in the `dist/` folder after building:

**macOS:**
- `OpenCoach-0.1.0-universal.dmg` - Drag-and-drop installer (Universal: Intel + Apple Silicon)
- `OpenCoach-0.1.0-universal-mac.zip` - Portable version

**Windows:**
- `OpenCoach Setup 0.1.0.exe` - Full installer with Start Menu shortcuts
- `OpenCoach 0.1.0.exe` - Portable executable

### Building Desktop Apps

```bash
# Build for macOS (requires macOS)
npm run electron:build:mac

# Build for Windows (can be built on macOS)
npm run electron:build:win

# Build for both platforms
npm run electron:build:all
```

**Note:** After building, the installers and executables will be available in the `dist/` directory.

### Development Mode

Run OpenCoach in Electron during development:

```bash
npm run electron:dev
```

üìö **For complete Electron documentation, see [ELECTRON.md](./ELECTRON.md)**

## Next Steps

- [ ] Integrate Mastra agent for advanced reasoning and tools
- [ ] Implement space concept and context files (CONTEXT.md)
- [ ] Add scheduled reminders and notifications
- [ ] Build multi-space support
- [ ] Enhanced note organization and search


